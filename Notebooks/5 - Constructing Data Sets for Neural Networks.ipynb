{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca53ac0-c6d8-4fe5-b915-52a7f3ac6613",
   "metadata": {},
   "source": [
    "# Constructing Data Sets for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f4f5b-01d6-42cd-9bd2-b8b08dde84bc",
   "metadata": {},
   "source": [
    "Author: Jake Dumbauld <br>\n",
    "Contact: jacobmilodumbauld@gmail.com<br>\n",
    "Date: 3.15.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690291d-5c05-4c93-9a38-a03a488c56a6",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b588a-1eb3-4c92-a374-51493fcb802a",
   "metadata": {},
   "source": [
    "The purpose of this notebook is make the most of the patient demographic information available in the data set that I have by converting variables into a machine-readable format. In addition, there are many subjective decisions made throughout on what to keep and what to throw away. I will explain my thought process as those decisions are being made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26531292-4883-44ea-869f-cc6ef3045131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import os\n",
    "from random import uniform\n",
    "import time\n",
    "import gc\n",
    "\n",
    "#options\n",
    "pd.set_option('display.max_columns', None) #making sure I can see all my columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbddd3f-c18e-4435-b0c2-4144b589757a",
   "metadata": {},
   "source": [
    "Picking up from my df created in `2 - Importing Signal Data` as the array created in 3 dropped all columns but the signal and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a859a372-ba47-4037-bf5a-352a026062d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/jmd/Documents/BOOTCAMP/Capstone/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec62ff3-0e23-4ded-9efa-6d9fa6d9db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = np.load(root_path+'arrays/patient_signals_4k.npy', allow_pickle=True),\n",
    "                       columns=(['Patient ID', 'Locations', 'Age', 'Sex', 'Height', 'Weight',\n",
    "                                 'Pregnancy status', 'Murmur', 'Murmur locations',\n",
    "                                 'Most audible location', 'Systolic murmur timing',\n",
    "                                 'Systolic murmur shape', 'Systolic murmur grading',\n",
    "                                 'Systolic murmur pitch', 'Systolic murmur quality',\n",
    "                                 'Diastolic murmur timing', 'Diastolic murmur shape',\n",
    "                                 'Diastolic murmur grading', 'Diastolic murmur pitch',\n",
    "                                 'Diastolic murmur quality', 'Campaign', 'Additional ID',\n",
    "                                 'location_count', 'signal_patient_id', 'location', 'signal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da32c74-a0a0-4eac-8b2e-5b379bcbdfb8",
   "metadata": {},
   "source": [
    "## Dropping duplicated or unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3fae4-a131-4e6a-ab88-5c15958ec16d",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f40362-f72e-4044-bdd2-bc51d4695aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Patient ID', 'Locations', 'Age', 'Sex', 'Height', 'Weight',\n",
       "       'Pregnancy status', 'Murmur', 'Murmur locations',\n",
       "       'Most audible location', 'Systolic murmur timing',\n",
       "       'Systolic murmur shape', 'Systolic murmur grading',\n",
       "       'Systolic murmur pitch', 'Systolic murmur quality',\n",
       "       'Diastolic murmur timing', 'Diastolic murmur shape',\n",
       "       'Diastolic murmur grading', 'Diastolic murmur pitch',\n",
       "       'Diastolic murmur quality', 'Campaign', 'Additional ID',\n",
       "       'location_count', 'signal_patient_id', 'location', 'signal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea2515-5459-4a61-b27e-7631e9da989b",
   "metadata": {},
   "source": [
    "### Reasoning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215581d3-548d-41a0-8edc-872d8aa4facb",
   "metadata": {},
   "source": [
    "In general, my goal is to observe the impact of patient information on model performance. Additionally, I'm interested in studying information that would be easy to obtain in the field if a machine learning system for phonocardiogram classification was employed that also took in patient information. With this in mind, I'll proceed through each of the columns in the dataframe and state my reasoning for keeping them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63779566-e26a-48f2-9b29-7c4ce0db0dbf",
   "metadata": {},
   "source": [
    "- `Patient ID`: Kept for now,  to keep things straight in the process ahead. Later dropped in this notebook as it is not necessary for the model to know.\n",
    "- `Locations`: Drop. This is the column that the `location` column is based on and contains duplicate information.\n",
    "- `Age`, `Sex`, `Height`, `Weight`: Keep. These are the most basic bits of information that can help shape a patient profile. All are relatively easy to obtain.\n",
    "- `Pregnancy Status`: Keep. Pregnancy has an effect on heart sounds ([source](https://www.ahajournals.org/doi/10.1161/circulationaha.114.009029)), and thus it is a very important metric to track in any potential modelling.\n",
    "- `Murmur`: Keep. This is our target variable.\n",
    "- `Systolic murmur timing` - `Diastolic murmur quality`: Drop - I'll admit, dropping these hurts as I'm sure it was no small feat to put together this info. However, this information isn't useful in training a binary model on the presence or absence of a murmur. On a model with functionality of grading a murmur, it would be great! That's not the problem I'm trying to solve though. In the future, the qualifying and grading info could be useful in evaluating the type of murmurs most misclassified.\n",
    "- `Campaign & Additional ID` - Drop. There were two campaigns that gathered all of these data points, denoted in the `campaign` column. If a patient participated in both campaigns they have a value in the `Additional ID` column. I expect the variability in the patients heart rate & sounds from day to day to provide different _enough_ data for the purpose of training a neural network, so I will not be dropping observations with an Additional ID.\n",
    "- `location_count` - Drop. Artifact from data processing used to expand the dataframe. Has no use in modelling.\n",
    "- `signal_patient_id` - Drop. Same as above.\n",
    "- `location` - This one is interesting. I've explained in previous notebooks that the location of the heart recording has an impact on the heart sounds heard. Thus, it makes sense to me to keep this information in. However, I feel there's a case to be made that this could confuse the model if it learns patterns of murmurs only being audible at one spot if there is some underlying skew in the data that I'm not aware of. Despite this, the variability in heart sounds from location to location compelled me to keep this in. \n",
    "- `signal` - Keep. This is my data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecce9a9-3d9f-4886-aa52-72dfb2cad1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Locations','signal_patient_id', 'Murmur locations',\n",
    "                   'Most audible location', 'Systolic murmur timing',\n",
    "                   'Systolic murmur shape', 'Systolic murmur grading',\n",
    "                   'Systolic murmur pitch', 'Systolic murmur quality',\n",
    "                   'Diastolic murmur timing', 'Diastolic murmur shape',\n",
    "                   'Diastolic murmur grading', 'Diastolic murmur pitch',\n",
    "                   'Diastolic murmur quality', 'Campaign', 'Additional ID', 'location_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b919b9-6ce3-45f0-b220-91c45c7b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb656e3-9b12-4773-80b2-79f1b3b0f163",
   "metadata": {},
   "source": [
    "## Binarizing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac45820-fac8-4352-ad42-95a311d5bd14",
   "metadata": {},
   "source": [
    "With the hard decisions out of the way, now I was down to taking these columns and translating them into a machine readable format. The first step was to binarize my target in the same was as was done in notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbaba3a-a563-4d84-8b2f-05dc3354d8be",
   "metadata": {},
   "source": [
    "### Binarizing Murmurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0e4fa5-81d9-4d25-af23-b0851de29982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we have any NaN's\n",
    "df['Murmur'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43a0f04-cd1c-4951-ada5-7e4936d0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Murmur'] == 'Unknown'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6bffe8-cec4-446c-9790-eb717c598e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Absent     2391\n",
       "Present     616\n",
       "Name: Murmur, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Murmur'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc67d630-bc46-4d15-9d91-ffc926b09f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Murmur'] = df['Murmur'].map({\"Absent\": 0,\n",
    "                                 \"Present\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b85985-6eb7-49e3-a0a9-1d4379e79b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6758d6-fe67-450d-a6bc-33043bdaaada",
   "metadata": {},
   "source": [
    "### Binarizing Pregnancy Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11f90f-ed26-421f-b75a-ee9ff1eedfed",
   "metadata": {},
   "source": [
    "Next, pregnancy status was given as a boolean, so it was as simple as casting it to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89e5720-a62f-47f1-8c39-6c15ced2c4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we have any NaN's\n",
    "df['Pregnancy status'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f19e4b-7930-48e1-9110-9043c2861d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pregnancy status'] = df['Pregnancy status'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c5d99-9860-4258-91d3-b6e1b1f315a3",
   "metadata": {},
   "source": [
    "### Binarizing Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e16d44-34b9-4a82-9d94-ca99c8930263",
   "metadata": {},
   "source": [
    "Patient sex was recorded as a string, so I arbitrarily mapped it to 0 for male and 1 for female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c833b0-1165-403b-be9e-5c6a0fafab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we have any NaN's\n",
    "df['Sex'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a5e755-cf8f-4daa-927a-3a1152dd8767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female    1523\n",
       "Male      1484\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe911ff-2904-4ae8-9258-ec7b01db7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].map({\"Male\": 0,\n",
    "                           \"Female\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a64220-84f7-4dd1-9415-ed0389511a13",
   "metadata": {},
   "source": [
    "### Mapping Age to Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c932e5-aea4-42e1-917a-ce4d51c4a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child          2125\n",
       "Infant          383\n",
       "Adolescent      230\n",
       "Young Adult      24\n",
       "Neonate           8\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53f8361-a19d-481d-a645-9e7f2bba06e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we have any NaN's\n",
    "df['Age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022e933-cec6-48a5-98ca-3eda02625fa0",
   "metadata": {},
   "source": [
    "This was the first real choice in this process. Age was given as categorical strings, and there were some unknowns present in the data. For the knowns, I decided to map them to ordinal values, increasing with patient age. For the `nan`s, Rather than imputing something potentially incorrect for the unknown values, I opted to map them all to zero. This is admittedly something I'm uncertain if I handled correctly, and could be explored in future studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83272ae8-024a-4aac-b524-18bcd0f609f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].map({np.nan: 0,\n",
    "                           'Neonate': 1,\n",
    "                           'Infant': 2,\n",
    "                           'Child': 3,\n",
    "                           'Adolescent': 4,\n",
    "                           'Young Adult': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee9d9b20-39b4-4a33-a3ac-8f28aaa40e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2125\n",
       "2     383\n",
       "0     237\n",
       "4     230\n",
       "5      24\n",
       "1       8\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9bc9f-e22e-4d66-988a-4a42c8b82389",
   "metadata": {},
   "source": [
    "### Dummy variables for location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be286d2a-0deb-4a13-9afb-fb72fe541ce5",
   "metadata": {},
   "source": [
    "Lastly, there were 5 locations from which recordings were taken. Since this is a categorical variable with only a few possibilities, I opted to one hot encode them use pandas build in `get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a5d0cd3-bda4-46d4-8be0-06530e3ce532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ebcc61b-4c67-4d88-934e-a7dd09eee302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0eb0d-77f0-4907-b412-2424364b8de2",
   "metadata": {},
   "source": [
    "I also wanted to have my signal data be the last column in the dataframe, for readability purposes during this process. There were a _lot_ of `df` calls on the last line of these cells to take a look at what I was doing and ensure I wasn't missing anything. These have been cleaned up for the reader :). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d79419-3706-4c39-b464-1f35faac1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_col = df.pop('signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d366989-8efb-423f-b352-9e84d7713985",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_position = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3743c4c-0487-445f-9d8d-00e456025dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(last_position, 'signal', last_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbcd39-e392-46cb-98dd-473265fa5263",
   "metadata": {},
   "source": [
    "### Dealing with NaNs in Height/Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbb3f4-cbf1-4ba8-99d1-5be125e2345b",
   "metadata": {},
   "source": [
    "The second major choice in this process: I had a few hundred `nan` `height` and `weight` values, what to do with them? </br></br>\n",
    "I opted to impute these values with the means of same sex/age groups. For most, this worked. However, for some patients there was no age information so I was unable to impute these values. Instead, I chose to impute the mean of the whole sample rather than setting them to zero. This is obviously not ideal, but with limited information it was the best I could do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e447530-f00a-4bae-814d-5cc08f99b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with NaN heights\n",
    "new_heights = []\n",
    "\n",
    "#iterating through heights\n",
    "for i, height in enumerate(df['Height']):\n",
    "    \n",
    "    #if the height is null \n",
    "    if (pd.isnull(height) == True):        \n",
    "        \n",
    "        #store the age and sex groups of the current patient in memory\n",
    "        age_group = df.iloc[i,1]\n",
    "        sex_group = df.iloc[i,2]\n",
    "        \n",
    "        #creating a condition that checks if age and sex are equal to the age and sex group\n",
    "        condition = (df['Age'] == age_group) & (df['Sex'] == sex_group)\n",
    "        \n",
    "        #computing the mean height of the patient group with same age and sex category\n",
    "        groups_height_mean = df[condition]['Height'].mean()\n",
    "        \n",
    "        #if that mean is null append the mean of the entire sample\n",
    "        if (pd.isnull(groups_height_mean) == True):\n",
    "            \n",
    "            new_heights.append(df['Height'].mean().round(1))\n",
    "        \n",
    "        #else append the group mean\n",
    "        else:\n",
    "            \n",
    "            new_heights.append(groups_height_mean.round(1))\n",
    "    \n",
    "    else:\n",
    "        new_heights.append(df['Height'][i])\n",
    "\n",
    "# reassigning heights with imputation.\n",
    "df['Height'] = new_heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231149f7-b099-484f-8b07-aad658b416fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with NaN weights\n",
    "new_weights = []\n",
    "\n",
    "#iterating through weights\n",
    "for i, weight in enumerate(df['Weight']):\n",
    "    \n",
    "    #if the weight is null \n",
    "    if (pd.isnull(weight) == True):        \n",
    "        \n",
    "        #store the age and sex groups of the current patient in memory\n",
    "        age_group = df.iloc[i,1]\n",
    "        sex_group = df.iloc[i,2]\n",
    "        \n",
    "        #creating a condition that checks if age and sex are equal to the age and sex group\n",
    "        condition = (df['Age'] == age_group) & (df['Sex'] == sex_group)\n",
    "        \n",
    "        #computing the mean weight of the patient group with same age and sex category\n",
    "        groups_weight_mean = df[condition]['Weight'].mean()\n",
    "        \n",
    "        #if that mean is null append the mean of the entire sample\n",
    "        if (pd.isnull(groups_weight_mean) == True):\n",
    "            \n",
    "            new_weights.append(df['Weight'].mean().round(1))\n",
    "        \n",
    "        #else append the group mean\n",
    "        else:\n",
    "            \n",
    "            new_weights.append(groups_weight_mean.round(1))\n",
    "    \n",
    "    else:\n",
    "        new_weights.append(df['Weight'][i])\n",
    "        \n",
    "# reassigning weights with imputation.\n",
    "df['Weight'] = new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11d390-3d7a-48d2-916a-2e4cdd187d94",
   "metadata": {},
   "source": [
    "Another two quick sanity checks here, since these were the last two I kept them in so the end result was visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14027932-8f78-41d2-9247-0832237addcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient ID          0\n",
       "Age                 0\n",
       "Sex                 0\n",
       "Height              0\n",
       "Weight              0\n",
       "Pregnancy status    0\n",
       "Murmur              0\n",
       "location_AV         0\n",
       "location_MV         0\n",
       "location_PV         0\n",
       "location_Phc        0\n",
       "location_TV         0\n",
       "signal              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2012f03f-6a5e-4f55-bd39-699ebd7e6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Pregnancy status</th>\n",
       "      <th>Murmur</th>\n",
       "      <th>location_AV</th>\n",
       "      <th>location_MV</th>\n",
       "      <th>location_PV</th>\n",
       "      <th>location_Phc</th>\n",
       "      <th>location_TV</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2530</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.07682987, 0.06061038, 0.039170958, 0.048250...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2530</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.01187718, 0.029969877, 0.01927742, -0.0206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2530</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.37442628, 0.32439327, 0.095518045, -0.06558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2530</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.06770988, 0.073658854, 0.072224066, 0.08253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9979</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.15039496, 0.18560724, 0.17212218, 0.1603406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>85345</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.03814805, 0.06008572, 0.03808801, -0.008758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>85345</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0061015976, 0.029588033, 0.020953469, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>85349</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.00026825635, 0.0034792388, 0.014762115, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>85349</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1387334, 0.08302983, 0.13867667, -0.0078439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>85349</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.20991552, 0.118048884, -0.0015913057, 0.006...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3007 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Patient ID  Age  Sex  Height  Weight  Pregnancy status  Murmur  \\\n",
       "0          2530    3    1    98.0    15.9                 0       0   \n",
       "1          2530    3    1    98.0    15.9                 0       0   \n",
       "2          2530    3    1    98.0    15.9                 0       0   \n",
       "3          2530    3    1    98.0    15.9                 0       0   \n",
       "4          9979    3    1   103.0    13.1                 0       1   \n",
       "...         ...  ...  ...     ...     ...               ...     ...   \n",
       "3002      85345    3    1   132.0    38.1                 0       0   \n",
       "3003      85345    3    1   132.0    38.1                 0       0   \n",
       "3004      85349    0    1   115.9    25.1                 1       0   \n",
       "3005      85349    0    1   115.9    25.1                 1       0   \n",
       "3006      85349    0    1   115.9    25.1                 1       0   \n",
       "\n",
       "      location_AV  location_MV  location_PV  location_Phc  location_TV  \\\n",
       "0               0            0            1             0            0   \n",
       "1               1            0            0             0            0   \n",
       "2               0            1            0             0            0   \n",
       "3               0            0            0             0            1   \n",
       "4               0            1            0             0            0   \n",
       "...           ...          ...          ...           ...          ...   \n",
       "3002            1            0            0             0            0   \n",
       "3003            0            0            1             0            0   \n",
       "3004            1            0            0             0            0   \n",
       "3005            0            0            1             0            0   \n",
       "3006            0            0            0             0            1   \n",
       "\n",
       "                                                 signal  \n",
       "0     [0.07682987, 0.06061038, 0.039170958, 0.048250...  \n",
       "1     [-0.01187718, 0.029969877, 0.01927742, -0.0206...  \n",
       "2     [0.37442628, 0.32439327, 0.095518045, -0.06558...  \n",
       "3     [0.06770988, 0.073658854, 0.072224066, 0.08253...  \n",
       "4     [0.15039496, 0.18560724, 0.17212218, 0.1603406...  \n",
       "...                                                 ...  \n",
       "3002  [0.03814805, 0.06008572, 0.03808801, -0.008758...  \n",
       "3003  [-0.0061015976, 0.029588033, 0.020953469, 0.00...  \n",
       "3004  [0.00026825635, 0.0034792388, 0.014762115, -0....  \n",
       "3005  [0.1387334, 0.08302983, 0.13867667, -0.0078439...  \n",
       "3006  [0.20991552, 0.118048884, -0.0015913057, 0.006...  \n",
       "\n",
       "[3007 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3186-73c9-4744-8952-ba03cbe1d9af",
   "metadata": {},
   "source": [
    "From here, I saved just my target variable to a numpy array for easy loading in later notebooks, as you'll see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d90590-6058-4c4b-92cd-c000b27f9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_array = df['Murmur'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "067b0c27-f676-46a2-b041-49622066ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_path+'arrays/target_array', murmur_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917bd1e-fbd1-4e55-ba55-18d9990f31bb",
   "metadata": {},
   "source": [
    "### Trimming and Padding clips to 12 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad1dff-24cc-4705-aab8-efe338e2b391",
   "metadata": {},
   "source": [
    "Since in my initial loading I had loaded in the raw signal data that was untrimmed and processed, I repeated the trimming/padding code from notebook 3. Since this is identical, I'll be taking a brief break from markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb6602bc-ece2-4d6b-a02b-ce784b65ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f00ed94-c671-4dbd-a090-353e72bfdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = df['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb8e213-9ffc-498d-b906-f5505fcf450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for signal in signals:\n",
    "    lengths.append(len(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6a86634-745b-4ba3-b430-6874d1984dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a62ee601-a6ea-42a5-89d7-072fa62c08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     0.734131\n",
       "mean     22.894569\n",
       "std       7.297946\n",
       "min       5.152100\n",
       "25%      19.056152\n",
       "50%      21.488037\n",
       "75%      29.392090\n",
       "max      64.512207\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe() / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cb509ee-3f67-490f-bf76-2ef2d3eca0c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.0\n",
      "5 1.0\n",
      "6 0.9986697705354174\n",
      "7 0.9960093116062521\n",
      "8 0.9890256069171932\n",
      "9 0.9787163285666778\n",
      "10 0.9630861323578317\n",
      "11 0.9441303624875291\n",
      "12 0.9218490189557699\n",
      "13 0.8985700033255737\n",
      "14 0.8782840039906884\n",
      "15 0.8540073162620552\n",
      "16 0.8240771533089458\n",
      "17 0.79847023611573\n",
      "18 0.7751912204855338\n",
      "19 0.7509145327569006\n",
      "20 0.6508147655470569\n",
      "21 0.5340871300299301\n",
      "22 0.4672430994346525\n",
      "23 0.4286664449617559\n",
      "24 0.40239441303624873\n",
      "25 0.3831060857998005\n",
      "26 0.36647821749251747\n",
      "27 0.3518456933821084\n",
      "28 0.32790156301962087\n",
      "29 0.28633189225141337\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 30):\n",
    "    print(i, len(lengths[lengths > (i * sr)]) / len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80cdfc28-8d4f-4f62-aa2f-6cb626024d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_len = 12 * sr\n",
    "target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87ff1fc1-5d2d-4000-a53f-d6baac2ae17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007, 49152)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_signals = []\n",
    "for signal in signals:\n",
    "    if len(signal) == target_len:\n",
    "        new_signals.append(signal)\n",
    "    elif len(signal) > target_len:\n",
    "        new_signals.append(signal[0:target_len])\n",
    "    elif len(signal) < target_len:\n",
    "        padwidth = target_len-len(signal)\n",
    "        new_signals.append(np.pad(signal, (0, padwidth), mode='constant'))\n",
    "    else:\n",
    "        print('wtf')\n",
    "\n",
    "new_signals = np.asarray(new_signals)\n",
    "\n",
    "new_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3bdaf44-5cf0-477f-a8c8-ed898b0ce256",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for signal in new_signals:\n",
    "    lengths.append(len(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4136c9b-773b-48fa-9a6d-d96a91917a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc66a287-cef1-40aa-920c-4ca87daf90cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     0.734131\n",
       "mean     12.000000\n",
       "std       0.000000\n",
       "min      12.000000\n",
       "25%      12.000000\n",
       "50%      12.000000\n",
       "75%      12.000000\n",
       "max      12.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe() / sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2e214-6a27-4008-a5a3-066ff73fcad9",
   "metadata": {},
   "source": [
    "## Important Interlude: Taking Stock & Plotting the Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085db3f-0de6-4b7d-bef6-8f828588c013",
   "metadata": {},
   "source": [
    "Taking quick stock of relevant variables, \n",
    "- `df` is our dataframe with all of our patient demographic information + patient ID, and currently has the *unprocessed* (variable length) signal data in it. \n",
    "- `new_signals` is our list of properly trimmed signals. \n",
    "Taking stock of our goals:\n",
    "- The goal of the notebook was to create machine-readable data\n",
    "- The goal of this project is to evaluate different machine learning models and the effect of patient information on their output.\n",
    "\n",
    "At this point, I needed to decide what formats I wanted my data in. Throughout this process, I heavily referenced this [paper](https://www.mdpi.com/1099-4300/23/6/667/htm), and if you are exploring this space I encourage you to do so as well. What I landed on was creating 5 variants of my data to then feed into models. Already compelted was the 4k sampling rate (sr) raw signal data without patient information that I fed into the simple statistical models. The rest were as follows: \n",
    "- 1k sr signal data _without_ patient information\n",
    "- 1k sr signal data _with_ patient information\n",
    "- MFCC data derived from my 4k sr signal data _without_ patient information\n",
    "- MFCC data derived from my 4k sr signal data _with_ patient information.\n",
    "The reason for creating the 1k signal data is explained in the model search notebooks on RNN. </br></br>\n",
    "\n",
    "From this point, I needed to create MFCCs from the processed signals and find a way to merge in the relevant patient information. The shape of my MFCC output data from each observation (signal) in this dataset was `(20,97)`, and I had a vector of length 10 representing my patient demographic information. </br></br>\n",
    "After consulting with my instructors, I landed on taking my vector of patient demographic information and creating 'static signals' from it. I reshaped it to an array with shape `(10,1)`. I then repeated the values in this array across the length of my MFCC array to generate 'static signals' with a shape of `(10,97)` that represented my patient information. Then, I stacked the two arrays together to form a final array with shape `(30,97)`. The first 20 rows were my MFCC data signals, and the last 10 rows were my static patient demographic 'signals.' I did this iteratively for each observation in my dataset to generate a 3D array with shape `(3007, 30, 97)`. This contained MFCC & Patient information for all of the audio recordings in my dataset.</br></br>\n",
    "\n",
    "In addition, I needed to create an MFCC array without the patient information. This was simpler, and just required repeating the code from above without the inclusion of the demographic information. The final shape of this array would be `(3007, 20, 97)`, 20 rows and 97 observations for the MFCC data, for 3007 audio files. </br></br>\n",
    "\n",
    "The code that follows accomplishes the above goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca6433-ab4e-4f73-bbf4-28a5052c4abb",
   "metadata": {},
   "source": [
    "## MFCC Data w/wo Patient Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba947ec-3b53-4444-98f8-6343a880dac0",
   "metadata": {},
   "source": [
    "#### Variable Selection & Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f17863-060d-4f7d-bad1-48ced6e3b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 256\n",
    "n_mfcc = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8d8c4-d18b-460c-8e3c-91f28a6cf6ff",
   "metadata": {},
   "source": [
    "Recall in the Basic Librosa notebook what these variables mean - `n_fft` is our window, and `n_mfcc` is the number of 'bins' we're creating. Implicit in this is our `hop_length`, which by default is set to 1/4 of `n_fft`. I didn't make any adjustments to this. </br></br>\n",
    "\n",
    "The reasoning behind shortening our `n_fft` window lies in our low sr. Typical approaches using MFCCs (speech recognition, music genre classification) have sr's of the standard 22khz or 44khz because they have frequencies in their data that require that high of a sampling rate, and thus the window can be much larger as there's many more data points per second available. The loss of granularity from the fft window sliding across the sound data is not pronounced. </br></br>\n",
    "\n",
    "However, we have a 4k sr. A window of 1k would mean that we were taking a power spectrum every 1/4th of a second, rather than every 1/20 or 1/40 seconds. To compensate for this, I reduced the window by a factor of four from the example in the librosa notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2cc8e-9250-4769-bc9f-debb8f72315a",
   "metadata": {},
   "source": [
    "### Transforming Signal Array into MFCC Array With Patient Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed29e7-6f5c-4ebc-ba6a-b0313a690e87",
   "metadata": {},
   "source": [
    "To keep my code clean, I wrote a quick helper function that takes the demographic information from an observation at position `i` in `df` and returns it as an array in the shape specified above `(10,97)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a200963f-717f-41f1-8cec-d4ad18ce3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_info_to_signal(df, i, repeats):\n",
    "    '''\n",
    "    Helper function to take the patient demo info and reshape it into static signals with length equal to the \n",
    "    MFCC array it's being concatenated with.\n",
    "    df: input dataframe from where the demographic info is coming from\n",
    "    i: row number of target patient demo info\n",
    "    repeats: length of signal representing patient info\n",
    "    '''\n",
    "    \n",
    "    demo_info = df.drop(columns=['Patient ID', 'Murmur', 'signal']).iloc[i,:].to_numpy()\n",
    "\n",
    "    demo_info = demo_info.reshape(demo_info.shape[0],1)\n",
    "\n",
    "    demo_info = np.repeat(demo_info, repeats = repeats, axis=1)\n",
    "\n",
    "    return demo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d68ea-ca8a-4b5e-a3d2-e1ce07392933",
   "metadata": {},
   "source": [
    "I then looped through every observation in my new_signals list (which is the same length as `df`) and converted these to MFCCs. This needed to be broken up into two blocks as I couldn't find a clean way to build the final array (shape: `(3007, 30, 97)`) that I was trying to create without reshaping the first two outputs and stacking them, then concatenating them on the new axis I created in the reshaping process. The below block of code is the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84c635bb-05e3-40df-8660-74110ebae0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007, 30, 97)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,len(new_signals)):\n",
    "    if i == 0:\n",
    "        \n",
    "        #defines first MFCC from row 1 and concatenates with patient info signal\n",
    "        MFCCs = librosa.feature.mfcc(y = new_signals[i], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc)\n",
    "        demo_info = patient_info_to_signal(df, i, MFCCs.shape[1])\n",
    "        MFCCs_and_patient = np.concatenate((MFCCs, demo_info), axis=0)\n",
    "        \n",
    "        #defines second MFCC from row\n",
    "        MFCCs2 = librosa.feature.mfcc(y = new_signals[i+1], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc)\n",
    "        demo_info = patient_info_to_signal(df, i+1, MFCCs.shape[1])\n",
    "        MFCCs2_and_patient = np.concatenate((MFCCs2, demo_info), axis=0)\n",
    "        \n",
    "        #and this is why we have this whole block. choosing to use .stack to start building the final array\n",
    "        final_patient_MFCC = np.stack((MFCCs_and_patient, MFCCs2_and_patient))\n",
    "        \n",
    "    if i == 1:\n",
    "        continue\n",
    "        \n",
    "    elif i > 1:\n",
    "        #building another MFCC & patient signal \n",
    "        MFCCs = librosa.feature.mfcc(y = new_signals[i], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc) \n",
    "        demo_info = patient_info_to_signal(df, i, MFCCs.shape[1])\n",
    "        MFCCs_and_patient = np.concatenate((MFCCs, demo_info), axis=0)\n",
    "        MFCCs_and_patient = MFCCs_and_patient.reshape(1,MFCCs_and_patient.shape[0],MFCCs_and_patient.shape[1])\n",
    "        \n",
    "        #assembling the final array\n",
    "        final_patient_MFCC = np.concatenate((final_patient_MFCC, MFCCs_and_patient))\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    display(final_patient_MFCC.shape)\n",
    "    \n",
    "np.save(root_path+'arrays/MFCCs_withPatient', final_patient_MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858985d-5692-4f13-b47b-c0cce0dfddbe",
   "metadata": {},
   "source": [
    "Below block of code can be executed if you run into memory issues with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fecc5c60-785c-4cc6-9afa-c8b92b8c2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del final_patient_MFCC\n",
    "# gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8428a-a4b4-4b5a-8bee-df28de83fd88",
   "metadata": {},
   "source": [
    "### Without Patient Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083775b-3d53-4cc4-acbf-34f7e82496f5",
   "metadata": {},
   "source": [
    "I repeated the above block of code but without the inclusion of the helper function returning the demographic information to build out my MFCC data without the patient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "114dec3c-d089-4785-b06e-c8ac311ae0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007, 20, 97)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,len(new_signals)):\n",
    "    if i == 0:\n",
    "        \n",
    "        #defines first MFCC from row 1\n",
    "        MFCCs = librosa.feature.mfcc(y = new_signals[i], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc)\n",
    "        \n",
    "        #defines second MFCC from row\n",
    "        MFCCs2 = librosa.feature.mfcc(y = new_signals[i+1], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc)\n",
    "        \n",
    "        #and this is why we have this whole block. choosing to use .stack to start building the final array\n",
    "        final_MFCC = np.stack((MFCCs, MFCCs2))\n",
    "        \n",
    "    if i == 1:\n",
    "        continue\n",
    "        \n",
    "    elif i > 1:\n",
    "        #building another MFCC\n",
    "        MFCCs = librosa.feature.mfcc(y = new_signals[i], sr = sr, n_fft = n_fft, n_mfcc = n_mfcc) \n",
    "        MFCCs = MFCCs.reshape(1,MFCCs.shape[0],MFCCs.shape[1])\n",
    "        \n",
    "        #assembling the final array\n",
    "        final_MFCC = np.concatenate((final_MFCC, MFCCs))\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    display(final_MFCC.shape)\n",
    "    \n",
    "np.save(root_path+'arrays/MFCCs_noPatient', final_MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528d075-280d-4275-875c-a839eb936582",
   "metadata": {},
   "source": [
    "Below block of code can be executed if you run into memory issues with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf7e7c-0d4e-4769-a79d-625f1af4e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del final_MFCC\n",
    "# gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28689042-fc9e-4341-9dac-e2515c22789a",
   "metadata": {},
   "source": [
    "## Unprocessed signal data w/wo Patient Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a032f2-23ec-438f-ac41-216362fef9c5",
   "metadata": {},
   "source": [
    "Now, I needed to generate the 1k signal data with and without patient information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a861dc0-466d-4680-9962-35742e9f8398",
   "metadata": {},
   "source": [
    "First, I dropped the `signal` column from `df`, which contained the old, unprocessed, 4k signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27459680-5e43-4bdc-920f-59428b1be949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='signal', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1430dc-781b-4d8e-8383-3b252c25bd03",
   "metadata": {},
   "source": [
    "Then, I imported the 1k signal data df from notebook 2, and created a list with just the signal data from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc9470be-15ec-4999-8be7-07eed7d6866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.037114453, 0.061636038, 0.055389933, 0.0675...\n",
       "1       [0.006357202, -0.010564456, -0.008372305, 0.00...\n",
       "2       [0.16865823, 0.035755683, -0.01783402, 0.00613...\n",
       "3       [0.042926352, 0.08795212, 0.081163116, 0.07926...\n",
       "4       [0.11233253, 0.16617733, 0.22739325, 0.3153002...\n",
       "                              ...                        \n",
       "3158    [0.02705935, 0.0051991576, -0.028789269, -0.00...\n",
       "3159    [0.011014578, 0.009928619, 0.014445103, 0.0160...\n",
       "3160    [0.0010312841, -0.0074770185, 0.0053564664, 0....\n",
       "3161    [0.070928715, 0.053460453, -0.012949261, -0.09...\n",
       "3162    [0.07846564, 0.006256001, -0.02002155, 0.00305...\n",
       "Name: signal, Length: 3163, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_k_signals = pd.DataFrame(data = np.load('/Users/jmd/Documents/BOOTCAMP/Capstone/arrays/patient_signals_1k.npy', allow_pickle=True),\n",
    "                                   columns=(['Patient ID', 'Locations', 'Age', 'Sex', 'Height', 'Weight',\n",
    "                                             'Pregnancy status', 'Murmur', 'Murmur locations',\n",
    "                                             'Most audible location', 'Systolic murmur timing',\n",
    "                                             'Systolic murmur shape', 'Systolic murmur grading',\n",
    "                                             'Systolic murmur pitch', 'Systolic murmur quality',\n",
    "                                             'Diastolic murmur timing', 'Diastolic murmur shape',\n",
    "                                             'Diastolic murmur grading', 'Diastolic murmur pitch',\n",
    "                                             'Diastolic murmur quality', 'Campaign', 'Additional ID',\n",
    "                                             'location_count', 'signal_patient_id', 'location', 'signal']))\n",
    "one_k_signals = one_k_signals.iloc[:,-1]\n",
    "one_k_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb7e89-4184-4114-a963-5e0e5a260c36",
   "metadata": {},
   "source": [
    "I then inserted this new signal data back into my df so that I could reuse the code from notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1404569-a9a2-4fc9-88ae-ceca63204f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=len(df.columns), column='signal', value=one_k_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff669c58-20d3-44f3-b467-c2fc6b9738de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 1024\n",
    "\n",
    "signals = df['signal']\n",
    "\n",
    "target_len = 6 * sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637b89f-2c19-40ed-90a3-1acdeea33f1b",
   "metadata": {},
   "source": [
    "Another important decision was made here - for these signals only 6 second clips were used. This is due to the size of the 1k signal w/ patient info array, which ended up being a REAL memory hog. This will become important when evaluating the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af282b7e-7e28-4480-b4d0-a95429302621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building out array of padded and trimmed signals\n",
    "new_signals = []\n",
    "for signal in signals:\n",
    "    if len(signal) == target_len:\n",
    "        new_signals.append(signal)\n",
    "    elif len(signal) > target_len:\n",
    "        new_signals.append(signal[0:target_len])\n",
    "    elif len(signal) < target_len:\n",
    "        padwidth = target_len-len(signal)\n",
    "        new_signals.append(np.pad(signal, (0, padwidth), mode='constant'))\n",
    "\n",
    "new_signals = np.asarray(new_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f4f1d6a-d5fa-4331-bffd-1de267aacada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007, 6144)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_signals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ee663-8ec7-4e51-9ea1-56d612743bc1",
   "metadata": {},
   "source": [
    "`new_signals` now contains the padded and trimmed to 6 seconds _1k sampling rate_ signals. Note the second dim, down from 49k to 6k. This was one of the goal shapes for my data, so I saved it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4d5dba4-4a6f-4805-a9a9-7c16dff0909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_path+'arrays/signal_noPatient', new_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76dcad-1132-4098-b69c-83b63f2e9d48",
   "metadata": {},
   "source": [
    "I then reused the code from above, as well as the helper function `patient_info_to_signal` to create another array. This array has shape `3007, 11, 6144`: 3007 observations, 11 channels (1 signal, 11 static patient demo info), and 6144 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6a32687-277c-4528-be42-2bae2d9fdabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3007, 11, 6144)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,len(new_signals)):\n",
    "    if i == 0:\n",
    "        \n",
    "        #defines first MFCC from row 1 and concatenates with patient info signal\n",
    "        demo_info = patient_info_to_signal(df, i, new_signals[i].shape[0])\n",
    "        signal_and_patient = np.concatenate((new_signals[i].reshape(1,len(new_signals[i])), demo_info), axis=0)\n",
    "        \n",
    "        #defines second MFCC from row\n",
    "        demo_info2 = patient_info_to_signal(df, i+1, new_signals[i+1].shape[0])\n",
    "        signal_and_patient2 = np.concatenate((new_signals[i+1].reshape(1,len(new_signals[i+1])), demo_info), axis=0)\n",
    "        \n",
    "        #and this is why we have this whole block. choosing to use .stack to start building the final array\n",
    "        final_patient_signal = np.stack((signal_and_patient, signal_and_patient2))\n",
    "        \n",
    "    if i == 1:\n",
    "        continue\n",
    "        \n",
    "    elif i > 1:\n",
    "        #building another MFCC & patient signal \n",
    "        demo_info = patient_info_to_signal(df, i, new_signals[i].shape[0])\n",
    "        signal_and_patient = np.concatenate((new_signals[i].reshape(1,len(new_signals[i])), demo_info), axis=0)\n",
    "        signal_and_patient = signal_and_patient.reshape(1,signal_and_patient.shape[0],signal_and_patient.shape[1])\n",
    "        \n",
    "        #assembling the final array\n",
    "        final_patient_signal = np.concatenate((final_patient_signal, signal_and_patient))\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    display(final_patient_signal.shape)\n",
    "    \n",
    "np.save(root_path+'arrays/signal_withPatient', final_patient_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05015367-e31e-42d1-bd96-fa05fc0ac4b2",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373ae9e-aff5-4cc7-b6e7-8232577122c6",
   "metadata": {},
   "source": [
    "And that's it! All the arrays to feed into the models are generated. Importantly, this process was iterative. I returned to this notebook _frequently_ to modify and create new datasets, as well as to make sure I wasn't going insane with the naming of my files (something I did _not_ do well in this project). Quick recap of our data sets:\n",
    "- 4k sr raw signal data _without_ patient information\n",
    "- 1k sr signal data _without_ patient information\n",
    "- 1k sr signal data _with_ patient information\n",
    "- MFCC data derived from my 4k sr signal data _without_ patient information\n",
    "- MFCC data derived from my 4k sr signal data _with_ patient information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41ac7e-ef43-4fd6-a33f-367bfe56ee8e",
   "metadata": {},
   "source": [
    "The next block of notebooks are building up my models that I evaluate in the last notebook. **PLEASE READ THE INTRO IN NOTEBOOK 0**. Markdown in the Model Search notebooks is sparse, as the plan is laid out in the first and the others are quite repetitve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
