{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1aa955-2e4f-4544-95af-291da9a7842b",
   "metadata": {},
   "source": [
    "# CNN MFCC - No Patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b580db6-5446-4881-b072-3ec62b88b30f",
   "metadata": {},
   "source": [
    "Author: Jake Dumbauld <br>\n",
    "Contact: jacobmilodumbauld@gmail.com<br>\n",
    "Date: 3.15.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ea252-0f7b-461d-ac04-11fa07daa078",
   "metadata": {},
   "source": [
    "## Introduction: A failed experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea37ca-853e-455a-896c-4a8f328f7e6e",
   "metadata": {},
   "source": [
    "Oh, how I wish I had RNN results to share. In short, this was a failed experiment. </br>\n",
    "\n",
    "I ran these notebooks on google collab with minor modifications, and they took over 24 hours to train. By that point, only 28 trials had resulted. I hit my limit on the collab notebook runtime, and it timed out. I lost my variables, and the tuner search hadn't resulted. I tried to modify the tuner parameters, limiting the number of trials to 25 and epochs to 50 to salvage the search, but in doing so I mistakenly overwrote the directory that contained the resulting models & scores from the first search. This second search timed out _again_, but, this time I didn't overwrite the model search files saved on gdrive. But since the search hadn't completed, and it had only trained for half the the number of trials and epochs as the others, I opted to discard the results. It violated the constrained environment I had set up for my search at the outset, and any results that came from it would have to be taken with a grain of salt.</br>\n",
    "\n",
    "This was, without question, the biggest disappointment in the project for me. I will return to this after the program has completed and finish it, because I have a few questions I still want answers to. </br>\n",
    "\n",
    "These questions are discussed in the technical report, so I will spare re-writing them here. Notebook 8 is the same as this one, with different data loaded in and some variable names changed.</br>\n",
    "\n",
    "I chose to leave the code in this submission since quite a bit of time went into learning about LSTMs and trying to decide on a reasonable search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3d0c7-d2ee-4f4c-bdc2-a07e9a4853ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, GRU\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650f7f8-b4ef-4386-80ac-92d5d24d073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "python_random.seed(42)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#not sure if the below are necessary - leaving in to perhaps un-comment later.\n",
    "%env PYTHONHASHSEED=0\n",
    "%env CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906b44e-1d67-4998-8e05-222cbff4a2e6",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e46f85-187e-44df-863c-e39301f83ff5",
   "metadata": {},
   "source": [
    "### Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c21cdb-6faf-4be0-b96a-f3a32bb2441d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_model_loss(title, history):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Graphs training vs validation loss over epochs for a given model. \n",
    "    \n",
    "    History: tensorflow.python.keras.callbacks.History object\n",
    "    Title: str\n",
    "    \"\"\" \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title,size=24)\n",
    "    plt.ylabel('Loss',size=16)\n",
    "    plt.xlabel('Epoch',size=16)\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1a723-2fd9-4474-aa34-76b70849b7d9",
   "metadata": {},
   "source": [
    "### Train/Test Acc Printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58477249-04e7-4e47-909d-e155dffc1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Outputs model train & test accuracies for currently defined train and test set variables.\n",
    "    \n",
    "    model: tensorflow model,\n",
    "    history: tensorflow.python.keras.callbacks.History object\n",
    "    \"\"\"\n",
    "    # Evaluate the network\n",
    "    train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "    result = model.evaluate(X_test,y_test, verbose=1)\n",
    "\n",
    "    print(f\"Train Accuracy: {np.round(train_accuracy, 6)*100}%\")\n",
    "    print(f\"Test Accuracy: {np.round(result[1], 6)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1e88c-a058-4c06-8d71-bc43beb4f98d",
   "metadata": {},
   "source": [
    "### Defining Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfb00d-edd8-434d-b157-d34ce333ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        LSTM(units=hp.Int('LSTM_units1', min_value=16, max_value=32, step=8),\n",
    "        activation='relu',\n",
    "        return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    #Tuning whether or not to use dropout.\n",
    "    if hp.Boolean(\"LSTM_dropout1\"):\n",
    "        model.add(Dropout(rate=0.25))\n",
    "                  \n",
    "    model.add(\n",
    "        LSTM(units=hp.Int('LSTM_units2', min_value=8, max_value=16, step=4),\n",
    "        activation='relu',\n",
    "        return_sequences=False\n",
    "        )\n",
    "    )\n",
    "    #Tuning whether or not to use dropout.\n",
    "    if hp.Boolean(\"LSTM_dropout2\"):\n",
    "        model.add(Dropout(rate=0.25))\n",
    "            \n",
    "    for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "            #Tuning the number of units in my input layer.\n",
    "            units=hp.Int(\"units\" + str(i), min_value=16, max_value=64, step=8),\n",
    "            activation=\"relu\"\n",
    "            )\n",
    "        )\n",
    "        #Tuning whether or not to use dropout.\n",
    "        if hp.Boolean(\"dropout\" + str(i)):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "\n",
    "        #Turning whether or not to add batch normalization\n",
    "        if hp.Boolean(\"normalization\" + str(i)):\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    #defining learning rate\n",
    "    lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                      #tuning initial learning rate\n",
    "                      initial_learning_rate=hp.Float(\"starting_learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
    "                      decay_steps=1.0,\n",
    "                      decay_rate=0.1\n",
    "                  )\n",
    "    model.compile(\n",
    "        #Optimizer\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        #Loss\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        #Metrics\n",
    "        metrics=[keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "build_RNN_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8de08-cd2c-4f92-a5db-a9ff06002f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('/Users/jmd/Documents/BOOTCAMP/Capstone/arrays/signal_noPatient.npy', allow_pickle=True)\n",
    "y = np.load('/Users/jmd/Documents/BOOTCAMP/Capstone/arrays/target_array.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9deda6-74bb-47c7-aa6c-c65a7b17e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e36874-5973-4b1e-8868-9a41a952220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09376a-5117-4284-82d9-26d2691cfd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2320cb-2a37-4998-a59e-2dca46250dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_RNN_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=25,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    directory='/Users/jmd/Documents/BOOTCAMP/Capstone/kerastune_searches',\n",
    "    project_name='RNN_signal_no_patient'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_val,y_val), callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2ae56-1b3c-4bbd-ba84-908cb0449261",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b113d2-7b01-4ae1-87cc-d10009988263",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc2612-8926-42ec-88f1-3ccce67d61e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the best hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "# Build the model with the best hp.\n",
    "best_model = build_model(best_hps[0])\n",
    "\n",
    "history = best_model.fit(X_train, y_train, epochs=100, validation_data=(X_val,y_val), callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60d415-bbc9-4f08-96c9-a5f54cb61527",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6504a12-ffdb-44bc-ae40-47030a1f8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9cdde7-dbc6-4f0f-b958-364bf6a151ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model_loss('LSTM Signal w/o Patient Information', sequential_MFCC_with_patient_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4a68c-3cab-4db1-80e1-a46707c61bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "sequential_MFCC_with_patient.save('/Users/jmd/Documents/BOOTCAMP/Capstone/neural_nets/LSTM_signal_no_patient', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
