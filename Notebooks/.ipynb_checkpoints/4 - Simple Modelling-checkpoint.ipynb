{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2461c2d-de76-4e68-8e3c-7528848fd480",
   "metadata": {},
   "source": [
    "# Simple Modelling on Phonocardiogram Murmurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0fe33-145b-4aaf-8fa9-4f4b3607b72b",
   "metadata": {},
   "source": [
    "Author: Jake Dumbauld <br>\n",
    "Contact: jacobmilodumbauld@gmail.com<br>\n",
    "Date: 3.15.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025f2d7-f44a-40dc-97c0-4c031d1feb8b",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1eae4-61c4-4186-8757-3345847fac14",
   "metadata": {},
   "source": [
    "In this notebook, I applied simple statistical modelling tools with skLearn to the data I created in the last notebook. Frankly, I wasn't expecting much; sequence data is a notoriously poor fit for these kinds of models. But I was curious to see if I could extract any kind of performance with the pre-processing I had done so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd155f5-b168-48bc-be6c-6e942a046342",
   "metadata": {},
   "source": [
    "## Importing Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2f93eb-13d3-4938-845f-7053b7b90774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Sklearn modules\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a44173-6a74-4a77-adf9-84eaff8f8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/jmd/Documents/BOOTCAMP/Capstone/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e82bc7-7dfc-453e-ba16-883e48d32c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = np.load(root_path + 'arrays/signal_murmur_prelogreg_4k.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cc005-5c10-4ec1-8e44-e762019efc2e",
   "metadata": {},
   "source": [
    "After importing the df array I created at the end of the last notebook, I renamed the first column, the target column, to murmur and did a quick check on the value counts to make sure it was in fact what I thought it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bea6e01-02e6-4890-98b4-82ad5bc38bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0:'Murmur'}, inplace=True)\n",
    "df['Murmur'] = df['Murmur'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6377823c-3e23-4d67-b3a4-a61b82db0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Murmur']\n",
    "X = df.drop('Murmur', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4781f96e-3cbd-4539-b2ea-4dc88beb4f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2391\n",
       "1     616\n",
       "Name: Murmur, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a06a0-9c31-4733-93b3-6d97167a965e",
   "metadata": {},
   "source": [
    "This was consistent with the values from the previous notebook, so I proceeded to do a train test split. Importantly, I stratified for y here to account for the fact that there was a class imbalance in my data. This ensures that there is roughly equal proportions of pos/neg target variable in my train and test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878e7ddf-6726-43b2-b76a-2bed5b9ce6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e64ba-0e18-4da6-9371-e0f6a2ac445a",
   "metadata": {},
   "source": [
    "## Evaluating Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a2f51-cba9-4cee-b069-f00b2257ea48",
   "metadata": {},
   "source": [
    "The first model I opted to test was a simple linear regression. Following the skLearn instantiate, fit, score approach below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c299f-abf7-4ea5-8b02-4f0ec16d524d",
   "metadata": {},
   "source": [
    "### Finding a baseline for a Simple LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b7d65c-1cf2-4f7c-b3ac-4d4885e37b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.81%\n",
      "Test Accuracy: 77.52%\n"
     ]
    }
   ],
   "source": [
    "# instantiate\n",
    "logit = LogisticRegression(solver='lbfgs', max_iter=100000)\n",
    "\n",
    "# fit\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "#score\n",
    "train_acc = logit.score(X_train, y_train)\n",
    "test_acc = logit.score(X_test,y_test)\n",
    "\n",
    "#report\n",
    "print(f\"Train Accuracy: {(train_acc*100).round(2)}%\")\n",
    "print(f\"Test Accuracy: {(test_acc*100).round(2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3d651-0c7b-4501-9684-f196ae3586e5",
   "metadata": {},
   "source": [
    "This model is clearly overfit, but when I first saw those results I was excited that I had a test accuracy of 77.52%. However, I quickly recalled that I had a significant class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580a5944-8797-4e03-84e5-2b3e31f9aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance of guessing correctly if you guess no every time: 74.23 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chance of guessing correctly if you guess no every time: \\\n",
    "{((1 - y_test.value_counts()[1]/y_test.value_counts()[0]) * 100).round(2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efab0c3-ff13-48c9-97c4-7414570cd8c7",
   "metadata": {},
   "source": [
    "And the wind was taken out of my sails. To nobody's surprise, logistic regression did not perform well on this data. From here, I could have done more in depth analysis looking at precision, recall, and f1 score, but I decided to save the in depth analysis for later, with the more advanced models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9661c-2501-4864-ad64-a5684c9f6339",
   "metadata": {},
   "source": [
    "### Finding a baseline for a Simple KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ed44d-dcf9-4287-a991-12d0e1ab45dc",
   "metadata": {},
   "source": [
    "From here, I tried KNN following the same process as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe0f51b-effb-4594-b71a-b91e883f24c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 79.61%\n",
      "Test Accuracy: 79.51%\n"
     ]
    }
   ],
   "source": [
    "#instantiate\n",
    "knnc = KNeighborsClassifier()\n",
    "\n",
    "#fit\n",
    "knnc.fit(X_train, y_train)\n",
    "\n",
    "#score\n",
    "train_acc = knnc.score(X_train, y_train)\n",
    "test_acc = knnc.score(X_test,y_test)\n",
    "\n",
    "#report\n",
    "print(f\"Train Accuracy: {(train_acc*100).round(2)}%\")\n",
    "print(f\"Test Accuracy: {(test_acc*100).round(2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d018c339-cb82-4467-81ae-7e818b7191ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': None,\n",
       " 'n_neighbors': 5,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9387675-69ae-4da4-941c-006575c6f3f6",
   "metadata": {},
   "source": [
    "This yielded similar results, though it was not as overfit and test accuracy was slightly higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca3529-a17f-44bd-b18f-24c21da96585",
   "metadata": {},
   "source": [
    "### Finding a Baseline for a Simple SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385e9aeb-a387-4d1d-bc96-2686d14a2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 87.69%\n",
      "Test Accuracy: 79.51%\n"
     ]
    }
   ],
   "source": [
    "#instantiate\n",
    "SVM = SVC()\n",
    "\n",
    "#fit\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "#score\n",
    "train_acc = SVM.score(X_train, y_train)\n",
    "test_acc = SVM.score(X_test,y_test)\n",
    "\n",
    "#report\n",
    "print(f\"Train Accuracy: {(train_acc*100).round(2)}%\")\n",
    "print(f\"Test Accuracy: {(test_acc*100).round(2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ec8f3-166f-4ff5-a7ef-a503f962d1ba",
   "metadata": {},
   "source": [
    "Again, same process for a support vector matrix. Train acc improved, but our test is the the same as our KNN. Since this appears to be more overfit to the data, I opted to pursue KNN further to see if I can squeeze any additional performance out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f0299-724b-451a-aac4-f576e827a257",
   "metadata": {},
   "source": [
    "## Pursuing KNN Further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43fd189-8c3e-44a4-89e0-9c7d1012441e",
   "metadata": {},
   "source": [
    "Below, I utilized a gridsearch to try and optimize a pipeline for my signal data into a KNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d42eb18-59a6-46b2-b703-4c104e5febf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('normalise', StandardScaler()),\n",
    "              ('reduce_dim', PCA()),\n",
    "              ('knn', KNeighborsClassifier())]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "param_grid = [\n",
    "            {'normalise': [StandardScaler(), None],\n",
    "             'reduce_dim': [PCA()],\n",
    "             'reduce_dim__n_components': [0.9],\n",
    "             'knn': [KNeighborsClassifier()], \n",
    "             'knn__n_neighbors': [5, 10, 25],\n",
    "             'knn__n_jobs': [-2]},\n",
    "            {'normalise': [StandardScaler(), None],\n",
    "             'knn': [KNeighborsClassifier()], \n",
    "             'knn__n_neighbors': [5, 10, 25],\n",
    "             'knn__n_jobs': [-2]\n",
    "            }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8d0855-93a5-4728-b3b1-8e6b2e5c37e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.784 total time=  19.4s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.779 total time=  22.3s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  21.7s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  20.4s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.776 total time=  18.4s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.8s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  18.9s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.791 total time=  18.7s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  16.9s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  16.6s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  19.8s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  19.0s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.798 total time=  17.5s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  17.9s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.795 total time=  18.4s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  16.7s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.6s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.7s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  17.5s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.795 total time=  16.7s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  18.1s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  18.3s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.8s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  19.4s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler(), reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.795 total time=  17.6s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.1s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  17.0s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.796 total time=  16.6s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.793 total time=  17.2s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None, reduce_dim=PCA(), reduce_dim__n_components=0.9;, score=0.795 total time=  17.0s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler();, score=0.796 total time=  18.1s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler();, score=0.796 total time=  18.3s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler();, score=0.796 total time=  17.9s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler();, score=0.793 total time=  18.1s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=StandardScaler();, score=0.795 total time=  18.9s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None;, score=0.796 total time=  17.0s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None;, score=0.796 total time=  16.9s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None;, score=0.796 total time=  17.3s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None;, score=0.793 total time=  17.4s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=5, normalise=None;, score=0.795 total time=  16.8s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler();, score=0.796 total time=  20.7s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler();, score=0.796 total time=  18.7s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler();, score=0.796 total time=  18.6s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler();, score=0.793 total time=  18.6s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=StandardScaler();, score=0.795 total time=  17.8s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None;, score=0.796 total time=  17.3s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None;, score=0.796 total time=  17.9s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None;, score=0.796 total time=  17.9s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None;, score=0.793 total time=  17.0s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=10, normalise=None;, score=0.795 total time=  16.6s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler();, score=0.796 total time=  17.8s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler();, score=0.796 total time=  18.1s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler();, score=0.796 total time=  17.9s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler();, score=0.793 total time=  17.8s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=StandardScaler();, score=0.795 total time=  18.3s\n",
      "[CV 1/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None;, score=0.796 total time=  17.8s\n",
      "[CV 2/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None;, score=0.796 total time=  17.1s\n",
      "[CV 3/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None;, score=0.796 total time=  17.6s\n",
      "[CV 4/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None;, score=0.793 total time=  17.2s\n",
      "[CV 5/5] END knn=KNeighborsClassifier(), knn__n_jobs=-2, knn__n_neighbors=25, normalise=None;, score=0.795 total time=  16.4s\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, param_grid, cv=5, verbose=4)\n",
    "fittedgrid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b82e0149-1910-4edb-9c15-f8a6d6c64f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalise', StandardScaler()),\n",
       "                ('reduce_dim', PCA(n_components=0.9)),\n",
       "                ('knn', KNeighborsClassifier(n_jobs=-2, n_neighbors=10))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedgrid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43421ef9-21b2-4690-a4e4-724845a3e682",
   "metadata": {},
   "source": [
    "My grid search yielded a pipleine of normalization, standard scaling, dimensionality reduction with principal component analysis, and n_neighbors of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732feb7-b71c-4d61-9c83-e00b39d65c23",
   "metadata": {},
   "source": [
    "### Best Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e40ef-5f76-4b25-92bb-176d0791b6f9",
   "metadata": {},
   "source": [
    "I instantiated the above best estimator pipeline below, and fit it to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a5a2ab-003e-49d9-9bd9-5e0cb307c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_jobs=-2, n_neighbors=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best estimator\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=0.9)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "#KNN\n",
    "best_knn = KNeighborsClassifier(n_neighbors=10, n_jobs=-2)\n",
    "best_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bcece7d-4c0e-4c82-a7fc-62f2b9bd37f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 79.52%\n",
      "Test Accuracy: 79.51%\n"
     ]
    }
   ],
   "source": [
    "#score\n",
    "train_acc = best_knn.score(X_train, y_train)\n",
    "test_acc = best_knn.score(X_test,y_test)\n",
    "\n",
    "#report\n",
    "print(f\"Train Accuracy: {(train_acc*100).round(2)}%\")\n",
    "print(f\"Test Accuracy: {(test_acc*100).round(2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f80e1-6432-430a-8110-d7956eb9b474",
   "metadata": {},
   "source": [
    "And got absolutely no performance improvement in the data. Unsurprising, but certainly a little disappointing given the runtime of gridsearch at ~20 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b8fd5-70c3-4b50-96c3-d1ceff3839a5",
   "metadata": {},
   "source": [
    "## Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac198ee5-c28f-4923-90d7-89c4c1870778",
   "metadata": {},
   "source": [
    "The results of this notebook were not surprising, in that simple statistical models performed poorly on complex signal data with minimal preprocessing. At this point I recognized that my problem would require employing neural networks, and for that I would need to shape my data differently. Moving on to notebook 5..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
